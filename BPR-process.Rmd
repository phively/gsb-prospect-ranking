---
title: "Booth Prospect Rankings process"
author: "Paul Hively"
date: "September 7, 2016"
output: html_document
---

This document describes the process for creating and updating the Booth Prospect Rankings. I do not cover model selection or attempt to justify the methods used.

# Quick version

To skip the modeling steps and refresh the scores only, do the following:

  1. [Setup](#setup) through "Preparing the data"
  2.

<a name="setup"></a>

# Setup

This demonstration uses data from the `All Booth Entities Modeling FY17` report, saved in the shared WebI folders `Biodata & Analytics\Engagement`.

The following resources are needed to follow along:

  * Installation of [R 3.2.0](https://cran.r-project.org/) or later
  * Installation of [RStudio](https://www.rstudio.com/products/RStudio/)
  * The files found in my [gsb-prospect-ranking](https://github.com/phively/gsb-prospect-ranking) GitHub repository -- use the [Clone or download](https://github.com/phively/gsb-prospect-ranking/archive/master.zip) link
  * This document, opened in RStudio
  * `ABE Modeling` tab from the `All Booth Entities Modeling FY17` report, saved as a .csv file

### Installing required R packages

My code makes use of several packages:

```{r see.libraries}
scan("LIBRARIES.txt", what="character")
```

This code will install and load them as necessary. First, install `devtools` and my `wranglR` package:

```{r get.wranglR}
# Check to see if devtools is unavailable
if (!("devtools" %in% utils::installed.packages()[, "Package"])) {
  # If unavailable, install it
  install.packages("devtools")
}

# Check to see if wranglR is unavailable
if (!("wranglR" %in% utils::installed.packages()[, "Package"])) {
  # If unavailable, use devtools to install it via GitHub
  devtools::install_github("phively/wranglR")
}
```

Now load `wranglR` and use it to install or load the other required libraries:

```{r load.libraries, warning=FALSE, message=FALSE}
# Load wranglR
library(wranglR)

# Feed the libraries specified in LIBRARIES.txt into wranglR's Libraries() function
Libraries(scan(file="LIBRARIES.txt", what="character"))
```

### Preparing the data

The following code assumes the `ABE Modeling` tab from `All Booth Entities Modeling FY17.csv` is saved in a `data` subfolder. Since the raw data includes fields like `Name` and `ID` we want to manually create factor levels, rather than have R automatically generate them. Also strip leading and trailing spaces.

```{r import.data, warning=FALSE}
# Import the data
full.data <- read.csv("data/ABE Modeling.csv", stringsAsFactors=FALSE, strip.white=TRUE) %>%
  # Drop any null rows
  filter(!is.na(Entity.ID))
```

The [following code](https://github.com/phively/gsb-prospect-ranking/blob/master/R/clean.data.R) creates a `dat` data frame from `full.data` -- the `source()` command runs the specified external file as an R script. It creates factors, converts strings to dates and numeric fields, and filters the data to only include non-duplicate alumni records.

```{r clean.data}
source("R/clean.data.R")
```

# Data exploration and variable selection

The cleaned data frame `dat` is substantial:

```{r data.size}
dim(dat)
```

Again, this is the cleaned up data file from a trimmed down report: I had 184 variables when first starting out on this project. Practically speaking, this is too large to fully explore and experiment with various combinations of variables, transformations, etc. I generally will look at the response variables and a couple key predictors I definitely want to include/exclude (based on domain knowledge) and then use an automatic variable selection routine to reduce the number of predictors.

### Response variable

I had previously decided to treat this as a classification problem. The response variable is `Gift.Donor.Flag..25k`, which is 1 for individuals who have ever made a single $25,000+ gift to Booth and 0 otherwise. Here's the frequency table, as well as a plot of the response against `Booth.ClassYr.or.RecYr`, a proxy for age (which is `NA` for a large proportion of the data).

```{r plot.response}
# Frequency table
table(dat$Gift.Donor.Flag..25k)

# Donor status versus year
dat %>%
  # Set up the axes
  ggplot(aes(x=Booth.ClassYr.or.RecYr)) +
  # Plot the donor and nondonor groups
  geom_density(alpha=.25, aes(color=Gift.Donor.Flag..25k, fill=Gift.Donor.Flag..25k)) +
  # Reference lines for the group means
  geom_vline(xintercept = dat %>% filter(Gift.Donor.Flag..25k=="Nondonor") %>% select(Booth.ClassYr.or.RecYr) %>% unlist() %>% mean(),
             linetype="dashed", color="pink") +
  geom_vline(xintercept = dat %>% filter(Gift.Donor.Flag..25k=="Donor") %>% select(Booth.ClassYr.or.RecYr) %>% unlist() %>% mean(),
             linetype="dashed", color="turquoise") +
  # Title and axis labels
  labs(title="Density plot of donor status by record year with group means", x="Year")
```

Only about `r {as.integer(dat$Gift.Donor.Flag..25k) %>% mean() - 1} %>% round(3) * 100 %>% I()`% of the alumni population has given at the $25,000+ level, and the donor group is about `r {dat %>% group_by(Gift.Donor.Flag..25k) %>% summarise(mean(Booth.ClassYr.or.RecYr))}[2] %>% unlist() %>% sort() %>% diff() %>% round(1) %>% I()` years older on average than the nondonor group.

In principle, this process should be repeated for the other key predictors to identify variables that need to be transformed before modeling. Additional visualizations like [partial residual plots](https://en.wikipedia.org/wiki/Partial_residual_plot) can be helpful in regression problems.

### Tree-based variable selection

Methods such as classification and regression trees (CART) and random forests have been proposed as a variable selection method, e.g. [Sauve & Tuleau-Malot, 2014](https://hal-unice.archives-ouvertes.fr/hal-00551375/document); [Genuer, Poggi, & Tuleau-Malot, 2009](https://hal.archives-ouvertes.fr/hal-00755489/file/PRLv4.pdf), and I find they have several appealing features.

  * Can easily test tens of thousands (!) of variables
  * No distribution assumptions
  * Works with missing data
  * Handles correlated predictors

```{r set.n, echo=FALSE}
# Number of rows to base variable selection on
n <- 10000
```

I'm trying out the `Boruta` package, which estimates variable importances across a large number of permuted datasets to distinguish between high and low importance features and noise. Here, I make the calculations based on a random sample of `r as.integer(n)` rows (around `r {n / nrow(dat) * 100} %>% round() %>% I()`% of the data).

**N.B. it is NOT recommended** to use the formula interface, i.e. `response ~ .`, with `randomForest`-based methods as this can cause severe performance degradation!

```{r boruta, cache=TRUE}
# Set seed for reproducible results
set.seed(34367)
samp <- sample_n(dat, size=n)

# Test the various features
(vars <- Boruta(y=samp$Gift.Donor.Flag..25k, x=select(samp, -Entity.ID, -Gift.Donor.Flag..25k), seed=63698))
```

Here's a visualization of the relative variable importances.

```{r boruta.viz, echo=FALSE, warning=FALSE}
# Construct initial data frame
ggdat <- data.frame(Importance=vars$ImpHistory) %>% gather("Variable", "Importance") %>%
  # Remove Importance. from the front of every variable name
  mutate(Variable=gsub("Importance.", "", Variable)) %>%
  # Append decision to the data frame
  left_join(data.frame(Decision=vars$finalDecision %>% relevel("Confirmed"),
                       Variable=names(vars$finalDecision)),
            by="Variable") %>%
  # Label shadow variables Reference
  mutate(Decision = factor(Decision, levels = c(levels(Decision), "Reference"))) %>%
  ReplaceValues(old.val=NA, new.val="Reference")

# Theme for importance plots
gg.rf.theme <- list(theme(axis.text.x=element_text(angle=90, hjust=1, vjust=.3),
                          panel.grid.minor=element_line(linetype="dotted")))

# Plot sorted standardized importances
ggdat %>% ggplot(aes(x=reorder(Variable, Importance), y=Importance, fill=Decision)) + geom_boxplot(alpha=.3) +
  gg.rf.theme + scale_fill_manual(values = c("green", "yellow", "red", "black")) +
  labs(title="Variable importances under Boruta algorithm", x="Variable", y="Importance")
```

The green boxes are confirmed to be important relative to reference "shadow" variables created by randomly permuting the predictors. Here they are, along with classification (`r sum(vars$finalDecision == "Confirmed") %>% I()` confirmed, `r sum(vars$finalDecision == "Tentative") %>% I()` tentative, `r sum(vars$finalDecision == "Rejected") %>% I()` rejected). Here are the confirmed variables:

```{r boruta.imp.yes}
# Find mean importances by variable
var.importance <- ggdat %>% group_by(Variable, Decision) %>% summarise(Importance = mean(Importance))

# Show all rows
options(tibble.print_max = Inf)

# Pick out confirmed important variables, and arrange by mean importance
var.importance %>% filter(Decision == "Confirmed") %>% select(Variable, Importance) %>% arrange(desc(Importance))
```

### The art and science of variable deletion

This is the point at which I think long and hard about what *really* should be included in the model. It depends on domain knowledge, results of data exploration, the form of model to be used, the model's goal, etc. In this context, I'm primarily concerned with identifying *new, high-potential* prospects. Practically speaking, this means:

  * Minimizing predictive error, e.g. finding the model $\text{argmin}_m \sum_i \left[ y_i - \widehat{m}_x(x_i) \right]^2$, on in-sample data is the *wrong* metric!
  * Focus on identifying as many current prospects as possible (minimizing type II error); type I is acceptable as these become new prospects.
  * Avoid overfitting the training data. Techniques like [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) are highly recommended.
  * Avoid [endogenous](https://en.wikipedia.org/wiki/Endogeneity_(econometrics)) variables; in this context, that means those that are causally associated with the outcome being measured, e.g. don't use `Lifetime.Giving` as a predictor if the response variable is `Largest.Gift`.

Some examples for this problem.

  1) `Action.NonVisit.Count..BUS.` is count of non-visit contacts with staff, and potentially problematic as it is a measure of cultivation, which is just one or two steps removed from giving.
  2) `Alloc.Stewardee.Student.Support` should absolutely not be used; stewardees are people who have given a large enough gift to receive a stewardship report!
  3) `Research.Non.Capacity` may be usable, as research indicates interest in cultivating someone, which is several steps removed from giving.
  4) `Action.Visit.Count..BUS.` is count of visit contacts with staff; again, this is a measure of cultivation and therefore problematic.
  5) `Giving.Ever.Pledged.to.Booth` may be usable. Even though most of the largest gifts are pledges, the vast majority of pledges are recurring annual gifts, a good indicator of donor engagement.
  6) `Giving.First.Trans.Amt` is the size of the first gift made; obviously problematic for non-donors and people who have only made a single gift, but some kind of indicator could be helpful. However, *beware the effect of inflation* -- a \$100 class gift from a 1960 graduate is worth about \$800 in 2015 dollars!

Taking out all the rejected variables and those that I deem too close to the response, and re-running the algorithm on the full dataset:

```{r boruta2, cache=TRUE}
# Variables to drop
rejected.vars <- var.importance %>% filter(Decision == "Rejected") %>% select(Variable) %>% unlist()
drop.vars <- c("Action.NonVisit.Count..BUS.", "Alloc.Stewardee.Student.Support", "Action.Visit.Count..BUS.", "Giving.AF.Scholarship", "Awards..BUS.", "Gift.Capacity.Numerical.Amt..CR.")

# Test the various features
(vars2 <- Boruta(y=dat$Gift.Donor.Flag..25k, x=select(dat, -one_of(rejected.vars), -one_of(drop.vars), -Entity.ID, -Gift.Donor.Flag..25k), seed=70639))
```
```{r boruta.viz2, echo=FALSE, warning=FALSE}
# Construct initial data frame
ggdat <- data.frame(Importance=vars2$ImpHistory) %>% gather("Variable", "Importance") %>%
  # Remove Importance. from the front of every variable name
  mutate(Variable=gsub("Importance.", "", Variable)) %>%
  # Append decision to the data frame
  left_join(data.frame(Decision=vars2$finalDecision %>% relevel("Confirmed"),
                       Variable=names(vars2$finalDecision)),
            by="Variable") %>%
  # Label shadow variables Reference
  mutate(Decision = factor(Decision, levels = c(levels(Decision), "Reference"))) %>%
  ReplaceValues(old.val=NA, new.val="Reference")

# Theme for importance plots
gg.rf.theme <- list(theme(axis.text.x=element_text(angle=90, hjust=1, vjust=.3),
                          panel.grid.minor=element_line(linetype="dotted")))

# Plot sorted standardized importances
ggdat %>% filter(Decision != "Reference") %>% ggplot(aes(x=reorder(Variable, Importance), y=Importance, fill=Decision)) + geom_boxplot(alpha=.3) +
  gg.rf.theme + scale_fill_manual(values = c("green", "yellow", "red", "black")) +
  labs(title="Variable importances under Boruta algorithm", x="Variable", y="Importance")
```

And there you have it -- `r sum(vars2$finalDecision == "Confirmed") %>% I()` of the remaining variables are important. This is a small enough set that they can be further reduced by standard methods as necessary.

# Modeling

Logistic regression is a proven classification method in donor identification problems, e.g. [Lindahl & Winship, 1994](https://www.jstor.org/stable/40196197?seq=1#page_scan_tab_contents); [Birkholz, 2008](http://www.wiley.com/WileyCDA/WileyTitle/productCd-047016557X.html) and the method I chose for this problem.

### Quick exploration

First, create the data frame we'll use for modeling. Note that I've dropped `Booth.Program.Group`, which is an aggregated version of `Booth.Program`.

```{r mdat}
# Create modeling data frame
mdat <- dat %>% select(-one_of(rejected.vars), -one_of(drop.vars), -Entity.ID, -Booth.Program.Group)

# Save headers to be used
write.table(colnames(mdat), file="results/mdat.vars.txt", row.names=FALSE, col.names=FALSE)

# Shuffle the data before previewing so that no rows are identifiable
set.seed(123)
mdat %>% sample_frac(size=1) %>% str()

# Variable summaries
summary(mdat)
```

A few things jump out.

  * Some of the factor levels could be combined, e.g. `Master.Addr.Type = Seasonal` only includes `r mdat %>% filter(Master.Addr.Type == "Seasonal") %>% nrow() %>% I()` people.
  * Looking at the quartiles, `Research.Non.Capacity`, `Giving.Ever.Pledged.to.Booth`, `Rel.Known.Tos.Count`, `Gift.Stock.Flag` and the various engagement indicators appear to be mostly 0s. They could be transformed or binned into factors.
  * From comparing the mean and median it's clear all of the various giving indicators are positively skewed, which follows given that they're left-bounded by 0. These should be transformed before use.

Let's look at a few examples.

#### Research.Non.Capacity

It's always a good idea to examine the variable's univariate distribution with a technique like a histogram or density estimate.

```{r research.non.cap.plot}
# Create the first plot; bins is equal to the max so each count gets its own bar (no binning)
(g <- mdat %>% ggplot(aes(x=Research.Non.Capacity)) + geom_histogram(bins=max(mdat$Research.Non.Capacity)))
```

It's a bit hard to see what's going on given the y scale, so let's transform it.

```{r research.non.cap.plot2, warning=FALSE}
# Add a log10 y scale to the plot
g + scale_y_log10()
```

The vast majority of individuals do not have any research. One possibility (and what I end up doing) is transforming this variable into an indicator that takes the value 1 if there is any research, e.g. $I(x) = \left\{ \begin{array}{ll} 1,\text{ x > 0} \\ 0,\text{ otherwise} \end{array} \right.$.

#### Giving.First.Trans.Amt

Temporarily dropping non-donors:

```{r giving.first.trans.amt.plot, warning=FALSE}
# Create the first plot
(g <- mdat %>% filter(Giving.First.Trans.Amt > 0) %>% ggplot(aes(x=Giving.First.Trans.Amt)) + geom_histogram(bins=20) +
  scale_x_log10(labels=scales::dollar))
```

This is nearly symmetric on the $log_{10}$ scale, but too correlated with lifetime giving to use directly. One possibility is to discretize it into quantiles.

```{r giving.first.trans.quantiles}
mdat %>% filter(Giving.First.Trans.Amt > 0) %>% select(Giving.First.Trans.Amt) %>% unlist() %>% quantile()
```

We know that only `r {as.integer(dat$Gift.Donor.Flag..25k) %>% mean() - 1} %>% round(3) * 100 %>% I()`% of constituents give at the target level, so the quantiles suggest it's reasonable to create a factor for first gift in the ranges $[0,25), ~ [25,50), ~ [50,100), ~ [100, \infty)$, which is what I end up doing.

### Data creation

The [following script](https://github.com/phively/gsb-prospect-ranking/blob/master/R/modeling.transformations.R) loads the data and performs the various transformations to be performed before modeling.

```{r mdat_script}
source("R/modeling.transformations.R")
```

Logistic regression is a [generalized linear model](https://en.wikipedia.org/wiki/Generalized_linear_model) based on the binomial distribution, which by default uses a logit link function. Here's the R implementation on all of the data:

```{r glm1}
glm1 <- mdat %>% glm(Gift.Donor.Flag..25k ~ ., data=., family=binomial())
summary(glm1) %>% print(digits=3)
```

The Estimate column gives coefficients for each variable, which can be used with the data to predict the log odds $logit(p) = log \left(\frac{p}{1-p} \right)$ that someone is a $25k+ donor in this case. The other three columns give a sense of how likely it is that the variable is "really" associated with the response, as opposed to being an artifact of this particular sample.

**N.B. dropping variables with high p-values is not a good method for refining a model!** Models should be assessed based on some objective function, which in this case is classification error.