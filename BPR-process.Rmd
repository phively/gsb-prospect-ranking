---
title: "Booth Prospect Rankings process"
author: "Paul Hively"
date: "September 7, 2016"
output: html_document
---

This document describes the process for creating and updating the Booth Prospect Rankings. I do not cover model selection or attempt to justify the methods used.

# Setup

This demonstration uses data from the `All Booth Entities Modeling FY17` report, saved in the shared WebI folders `Biodata & Analytics\Engagement`.

The following resources are needed to follow along:

  * Installation of [R 3.2.0](https://cran.r-project.org/) or later
  * Installation of [RStudio](https://www.rstudio.com/products/RStudio/)
  * The files found in my [gsb-prospect-ranking](https://github.com/phively/gsb-prospect-ranking) GitHub repository -- use the [Clone or download](https://github.com/phively/gsb-prospect-ranking/archive/master.zip) link
  * This document, opened in RStudio
  * `ABE Modeling` tab from the `All Booth Entities Modeling FY17` report, saved as a .csv file

### Installing required R packages

My code makes use of several packages:

```{r see.libraries}
scan("LIBRARIES.txt", what="character")
```

This code will install and load them as necessary. First, install `devtools` and my `wranglR` package:

```{r get.wranglR}
# Check to see if devtools is unavailable
if (!("devtools" %in% utils::installed.packages()[, "Package"])) {
  # If unavailable, install it
  install.packages("devtools")
}

# Check to see if wranglR is unavailable
if (!("wranglR" %in% utils::installed.packages()[, "Package"])) {
  # If unavailable, use devtools to install it via GitHub
  devtools::install_github("phively/wranglR")
}
```

Now load `wranglR` and use it to install or load the other required libraries:

```{r load.libraries, warning=FALSE, message=FALSE}
# Load wranglR
library(wranglR)

# Feed the libraries specified in LIBRARIES.txt into wranglR's Libraries() function
Libraries(scan(file="LIBRARIES.txt", what="character"))
```

### Preparing the data

The following code assumes the `ABE Modeling` tab from `All Booth Entities Modeling FY17.csv` is saved in a `data` subfolder. Since the raw data includes fields like `Name` and `ID` we want to manually create factor levels, rather than have R automatically generate them. Also strip leading and trailing spaces.

```{r import.data, warning=FALSE}
# Import the data
full.data <- read.csv("data/ABE Modeling.csv", stringsAsFactors=FALSE, strip.white=TRUE) %>%
  # Drop any null rows
  filter(!is.na(Entity.ID))
```

The [following code](https://github.com/phively/gsb-prospect-ranking/blob/master/R/clean.data.R) creates a `dat` data frame from `full.data` -- the `source()` command runs the specified external file as an R script. It creates factors, converts strings to dates and numeric fields, and filters the data to only include non-duplicate alumni records.

```{r clean.data}
source("R/clean.data.R")
```

# Data exploration and variable selection

The cleaned data frame `dat` is substantial:

```{r data.size}
dim(dat)
```

Again, this is the cleaned up data file from a trimmed down report: I had 184 variables when first starting out on this project. Practically speaking, this is too large to fully explore and experiment with various combinations of variables, transformations, etc. I generally will look at the response variables and a couple key predictors I definitely want to include/exclude (based on domain knowledge) and then use an automatic variable selection routine to reduce the number of predictors.

### Response variable

I had previously decided to treat this as a classification problem. The response variable is `Gift.Donor.Flag..25k`, which is 1 for individuals who have ever made a single $25,000+ gift to Booth and 0 otherwise. Here's the frequency table, as well as a plot of the response against `Booth.ClassYr.or.RecYr`, a proxy for age (which is `NA` for a large proportion of the data).

```{r plot.response}
# Frequency table
table(dat$Gift.Donor.Flag..25k)

# Donor status versus year
dat %>%
  # Set up the axes
  ggplot(aes(x=Booth.ClassYr.or.RecYr)) +
  # Plot the donor and nondonor groups
  geom_density(alpha=.25, aes(color=Gift.Donor.Flag..25k, fill=Gift.Donor.Flag..25k)) +
  # Reference lines for the group means
  geom_vline(xintercept = dat %>% filter(Gift.Donor.Flag..25k=="Nondonor") %>% select(Booth.ClassYr.or.RecYr) %>% unlist() %>% mean(),
             linetype="dashed", color="pink") +
  geom_vline(xintercept = dat %>% filter(Gift.Donor.Flag..25k=="Donor") %>% select(Booth.ClassYr.or.RecYr) %>% unlist() %>% mean(),
             linetype="dashed", color="turquoise") +
  # Title and axis labels
  labs(title="Density plot of donor status by record year with group means", x="Year")
```

Only about `r {as.integer(dat$Gift.Donor.Flag..25k) %>% mean() - 1} %>% round(3) * 100 %>% I()`% of the alumni population has given at the $25,000+ level, and the donor group is about `r {dat %>% group_by(Gift.Donor.Flag..25k) %>% summarise(mean(Booth.ClassYr.or.RecYr))}[2] %>% unlist() %>% sort() %>% diff() %>% round(1) %>% I()` years older on average than the nondonor group.

In principle, this process should be repeated for the other key predictors to identify variables that need to be transformed before modeling. Additional visualizations like [partial residual plots](https://en.wikipedia.org/wiki/Partial_residual_plot) can be helpful in regression problems.

### Tree-based variable selection

Methods such as classification and regression trees (CART) and random forests have been proposed as a variable selection method, e.g. [Sauve & Tuleau-Malot, 2014](https://hal-unice.archives-ouvertes.fr/hal-00551375/document); [Genuer, Poggi, & Tuleau-Malot, 2009](https://hal.archives-ouvertes.fr/hal-00755489/file/PRLv4.pdf), and I find they have several appealing features.

  * Can easily test tens of thousands (!) of variables
  * No distribution assumptions
  * Works with missing data
  * Handles correlated predictors

```{r set.n, echo=FALSE}
# Number of rows to base variable selection on
n <- 10000
```

I'm trying out the `Boruta` package, which estimates variable importances across a large number of permuted datasets to distinguish between high and low importance features and noise. Here, I make the calculations based on a random sample of `r as.integer(n)` rows (around `r {n / nrow(dat) * 100} %>% round() %>% I()`% of the data).

**N.B. it is NOT recommended** to use the formula interface, i.e. `response ~ .`, with `randomForest`-based methods as this can cause severe performance degradation!

```{r boruta, cache=TRUE}
# Set seed for reproducible results
set.seed(34367)
samp <- sample_n(dat, size=n)

# Test the various features
(vars <- Boruta(y=samp$Gift.Donor.Flag..25k, x=select(samp, -Entity.ID, -Gift.Donor.Flag..25k), seed=63698))
```

Here's a visualization of the relative variable importances.

```{r boruta.viz, echo=FALSE, warning=FALSE}
# Construct initial data frame
ggdat <- data.frame(Importance=vars$ImpHistory) %>% gather("Variable", "Importance") %>%
  # Remove Importance. from the front of every variable name
  mutate(Variable=gsub("Importance.", "", Variable)) %>%
  # Append decision to the data frame
  left_join(data.frame(Decision=vars$finalDecision %>% relevel("Confirmed"),
                       Variable=names(vars$finalDecision)),
            by="Variable") %>%
  # Label shadow variables Reference
  mutate(Decision = factor(Decision, levels = c(levels(Decision), "Reference"))) %>%
  ReplaceValues(old.val=NA, new.val="Reference")

# Theme for importance plots
gg.rf.theme <- list(theme(axis.text.x=element_text(angle=90, hjust=1, vjust=.3),
                          panel.grid.minor=element_line(linetype="dotted")))

# Plot sorted standardized importances
ggdat %>% ggplot(aes(x=reorder(Variable, Importance), y=Importance, fill=Decision)) + geom_boxplot(alpha=.3) +
  gg.rf.theme + scale_fill_manual(values = c("green", "yellow", "red", "black")) +
  labs(title="Variable importances under Boruta algorithm", x="Variable", y="Importance")
```

The green boxes are confirmed to be important relative to reference "shadow" variables created by randomly permuting the predictors. These are the ones determined to be important:

```{r boruta.imp.yes}
# Show all rows
options(tibble.print_max = Inf)
# Pick out confirmed important variables, and arrange by mean importance
ggdat %>% filter(Decision == "Confirmed") %>% group_by(Variable) %>% summarise(Importance = mean(Importance)) %>% arrange(desc(Importance)) %>% data.frame()
```

There were also `r sum(vars$finalDecision == "Tentative") %>% I()` variables classified `Tentative`:

```{r boruta.imp.maybe}
# Pick out confirmed important variables, and arrange by mean importance
ggdat %>% filter(Decision == "Tentative") %>% group_by(Variable) %>% summarise(Importance = mean(Importance)) %>% arrange(desc(Importance))
```

### The art and science of variable deletion

This is the point at which I think long and hard about what *really* should be included in the model. It depends on domain knowledge, results of data exploration, the form of model to be used, the model's goal, etc. In this context, I'm primarily concerned with identifying *new, high-potential* prospects. Practically speaking, this means:

  * Minimizing predictive error, e.g. finding the model $\text{argmin}_m \sum_i \left[ y_i - \widehat{m}_x(x_i) \right]^2$, on in-sample data is the *wrong* metric!
  * Focus on identifying as many current prospects as possible (minimizing type II error); type I is acceptable as these become new prospects.
  * Avoid overfitting the training data. Techniques like [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) are highly recommended.
  * Avoid [endogenous](https://en.wikipedia.org/wiki/Endogeneity_(econometrics)) variables; in this context, that means those that are causally associated with the outcome being measured, e.g. don't use `Lifetime.Giving` as a predictor if the response variable is `Largest.Gift`.

Some examples for this problem.

  1) `Action.NonVisit.Count..BUS.` is count of non-visit contacts with staff, and potentially problematic as it is a measure of cultivation, which is just one or two steps removed from giving.
  2) `Alloc.Stewardee.Student.Support` should absolutely not be used; stewardees are people who have given a large enough gift to receive a stewardship report!
  3) `Research.Non.Capacity` may be usable, as research indicates interest in cultivating someone, which is several steps removed from giving.
  4) `Giving.Ever.Pledged.to.Booth` may be usable. Even though most of the largest gifts are pledges, the vast majority of pledges are recurring annual gifts, a good indicator of donor engagement.

